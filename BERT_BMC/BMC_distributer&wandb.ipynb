{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05de151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597ab38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KoBERT.kobert.utils import get_tokenizer\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eac56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a49163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd0ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0024f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886e2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dfaf01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e44ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c123530",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model() \n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# gluonnlp.data.BERTSPTokenizer(path, vocab, num_best=0, alpha=1.0, lower=True, max_input_chars_per_word=200)\n",
    "# path : Path to the pre-trained subword tokenization model.\n",
    "# vocab : Vocabulary for the corpus.\n",
    "# num_best : default 0 – A scalar for sampling subwords. If num_best = {0,1}, no sampling is performed. If num_best > 1, \n",
    "#            then samples from the num_best results. If num_best < 0, then assume that num_best is infinite and samples \n",
    "#            from the all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.\n",
    "# alpha : A scalar for a smoothing parameter. Inverse temperature for probability rescaling.\n",
    "# lower :  default True) – Whether the text strips accents and convert to lower case. If you use the BERT pre-training model,\n",
    "#          lower is set to False when using the cased model, otherwise it is set to True.\n",
    "# max_input_dchars_per_word : default 200\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "358c2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cfee111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c5264cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "# batch_size = config.batch_size\n",
    "# num_epochs = config.epochs\n",
    "# learning_rate = config.learning_rate\n",
    "# drop_out_rate = config.dropout\n",
    "# test_ratio = config.test_ratio\n",
    "max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d0bd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 설정\n",
    "filename = 'processed_labelled_df'\n",
    "column_name = 'text'\n",
    "labels = '구체적 기재'\n",
    "# graph_title = '핵심자원'\n",
    "# image_name = '핵심자원'\n",
    "best_model_name = '구체적 기재_best_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0706b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, bert_tokenizer, max_len, pad, pair, column_name=column_name, labels=labels):\n",
    "        \n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        # BERTSentenceTransform\n",
    "        # https://nlp.gluon.ai/_modules/gluonnlp/data/transforms.html\n",
    "        # r\"\"\"BERT style data transformation.\n",
    "        # Parameters\n",
    "        #    ----------\n",
    "        #    tokenizer : BERTTokenizer.\n",
    "        #        Tokenizer for the sentences.\n",
    "        #    max_seq_length : int.\n",
    "        #        Maximum sequence length of the sentences.\n",
    "        #    vocab : Vocab\n",
    "        #        The vocabulary which has cls_token and sep_token registered.\n",
    "        #        If vocab.cls_token is not present, vocab.bos_token is used instead.\n",
    "        #        If vocab.sep_token is not present, vocab.eos_token is used instead.\n",
    "        #    pad : bool, default True\n",
    "        #        Whether to pad the sentences to maximum length.\n",
    "        #    pair : bool, default True\n",
    "        #        Whether to transform sentences or sentence pairs.\n",
    "        #    \"\"\"\n",
    "        # vocab 인자를 넣지 않는 이유는 알 수 없음\n",
    "        # https://nlp.gluon.ai/_modules/gluonnlp/data/transforms.html\n",
    "\n",
    "        sent_data = []\n",
    "    \n",
    "        for i in range(len(dataset)):\n",
    "            sent_data.append([str(dataset.iloc[i][column_name]), dataset.iloc[i][labels]])\n",
    "        # dataset의 text컬럼과 라벨컬럼을 리스트로 묶어서 sent_data에 더함\n",
    "        # 예) sent_data.append(['품질, 기술을 기반으로 반도체 및 디스플레이 생산에 효율과 수율을..., A])\n",
    "        # sent_data = [[text, label],[],[]...]\n",
    "        \n",
    "\n",
    "        self.sentences = [transform([i[0]]) for i in sent_data]\n",
    "        # sent_data의 text를 transform \n",
    "        # >array([  2, 993,   3,   1,   1,   1,  \n",
    "        # >1,   1,   1,   1,   1,   1,   1,   1,  \n",
    "        # >1,   1,   1,   1,   1,   1,   1,   1,  \n",
    "        # >1,   1,   1,   1,   1,   1,   1,   1,  \n",
    "        # >1,   1,   1,   1,   1,   1,   1,   1,\n",
    "        # >array(3, dtype=int32),\n",
    "        # >array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "        # > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "        # > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "        # > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        # > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "        \n",
    "        self.labels = [np.int32(i[1]) for i in sent_data]\n",
    "        # >array([0, 1, 0, 0, 0], dtype=int32),\n",
    "        # >array([0, 0, 1, 0, 0], dtype=int32),\n",
    "        # >array([0, 0, 0, 1, 0], dtype=int32),\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i] + (self.labels[i],)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=num_class,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),\n",
    "                              attention_mask=attention_mask.float().to(token_ids.device))\n",
    "\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        output = softmax(self.classifier(out))\n",
    "        # output = self.classifier(out)\n",
    "        # return self.classifier(out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48f0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path='./', filename=filename, labels=labels):\n",
    "    df = pd.read_csv(path + '/' + filename + '.csv', sep=',')\n",
    "    df = df.dropna(axis=0)\n",
    "\n",
    "    if num_class == 5:\n",
    "        df[labels] = df[labels].replace({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4})\n",
    "        df[labels] = pd.get_dummies(df[labels]).values[:, ::-1].tolist()  # one hot encoding\n",
    "    elif num_class == 2:\n",
    "        df = df.drop(df[df[labels] == 'C'].index)\n",
    "        df[labels] = df[labels].replace({'A': 1, 'B': 1, 'D': 0, 'E': 0})\n",
    "        df = df.dropna(axis=0)\n",
    "    elif num_class == 3:\n",
    "        # df[labels] = df[labels].replace({'A': 0, 'B': 0, 'C': 1, 'D': 2, 'E': 2})\n",
    "        # df['label'] = df['label'].replace({'A': 0, 'B': 1, 'C': 2})\n",
    "        df[labels] = df[labels].replace({0:0, 1:1, 2:2})\n",
    "        df[labels] = pd.get_dummies(df[labels]).values[:, ::-1].tolist()  # one hot encoding\n",
    "    else:\n",
    "        print('wrong input\\n')\n",
    "        sys.exit()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b840a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def one_hot_ce_loss(outputs, targets):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # nn.CrossEntropyLoss() : 다중 분류에 사용되는 손실함수, nn.LogSoftmax와 nn.NLLLoss의 연산의 조합\n",
    "    # http://www.gisdeveloper.co.kr/?p=8668 참고\n",
    "    \n",
    "    _, labels = torch.max(targets, dim=1)\n",
    "    # values, indices = torch.max(input, dim=1)\n",
    "    # values : 행에서 최대값\n",
    "    # indices : 최대값의 인덱스\n",
    "    # https://technical-support.tistory.com/94 참고\n",
    "    \n",
    "    return criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b63af0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38226<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/khd/BERT_BMC/wandb/run-20210919_025417-36wlm5yp/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/khd/BERT_BMC/wandb/run-20210919_025417-36wlm5yp/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">usual-water-22</strong>: <a href=\"https://wandb.ai/junoe/uncategorized/runs/36wlm5yp\" target=\"_blank\">https://wandb.ai/junoe/uncategorized/runs/36wlm5yp</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model(data=None, num_epochs=None, batch_size=None, test_ratio=None, drop_out_rate=None, learning_rate=None, graph=None, savemode=False, \n",
    "          column_name=column_name, labels=labels, graph_title=None, image_name=None, best_model_name=best_model_name):\n",
    "    \n",
    "    # wandb 초기값 설정\n",
    "    wandb.init(project='bax_KoBERT', entity='junoe',\n",
    "               config={\"learning_rate\": 1e-5,\n",
    "                        \"dropout\": 0.2,\n",
    "                        \"test_ratio\": 0.2,\n",
    "                        \"batch_size\": 4,\n",
    "                        \"epochs\": 5,\n",
    "                        \"architecture\": \"KoBERT\",\n",
    "                        \"dataset\": \"BMC 515\"})\n",
    "\n",
    "    config = wandb.config\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data[column_name],\n",
    "                                                        data[labels],\n",
    "                                                        test_size=config.test_ratio,\n",
    "                                                        shuffle=True,\n",
    "                                                        stratify=data[labels],\n",
    "                                                        random_state=42,\n",
    "                                                        )\n",
    "    df_train = pd.DataFrame()\n",
    "    df_train[column_name] = x_train\n",
    "    df_train[labels] = y_train\n",
    "    df_test = pd.DataFrame()\n",
    "    df_test[column_name] = x_test\n",
    "    df_test[labels] = y_test\n",
    "\n",
    "    model = BERTClassifier(bertmodel, dr_rate=config.dropout).to(device)\n",
    "\n",
    "    data_train = BERTDataset(df_train, tok, max_len, True, False)\n",
    "    data_test = BERTDataset(df_test, tok, max_len, True, False)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=config.batch_size, num_workers=4)\n",
    "    test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=config.batch_size, num_workers=4)\n",
    "    # data_train을 배치사이즈 만큼 나눠서(gpu에 할당) 모델에 데이터를 로드\n",
    "    # len(data_train) = 403\n",
    "    # len(traindataloader) = 202\n",
    "    # num_workers에 대한 설명 https://jybaek.tistory.com/799 참고\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n",
    "    # The optimizer allows us to apply different hyperpameters for specific parameter groups. \n",
    "    # For example, we can apply weight decay to all parameters other than bias and layer normalization terms\n",
    "    # 무슨 말인지 모르겠다...\n",
    "    # https://huggingface.co/transformers/v3.3.1/training.html 참고\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    t_total = len(train_dataloader) * config.epochs # 202 * 30\n",
    "    warmup_step = int(t_total * 0.1)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "    # Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, \n",
    "    # after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    # optimizer (Optimizer) – The optimizer for which to schedule the learning rate.\n",
    "    # num_warmup_steps (int) – The number of steps for the warmup phase.\n",
    "    # num_training_steps (int) – The total number of training steps.\n",
    "    # https://huggingface.co/transformers/main_classes/optimizer_schedules.html 참고\n",
    "\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    for e in range(config.epochs):\n",
    "        train_acc = 0.\n",
    "        test_acc = 0.\n",
    "        train_loss = 0.\n",
    "        test_loss = 0.\n",
    "        train_predict_list = []\n",
    "        train_actual_list = []\n",
    "        test_predict_list = []\n",
    "        test_actual_list = []\n",
    "\n",
    "        model.train()\n",
    "        # 훈련모드로 변경\n",
    "\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "            # batch_id : 배치사이즈\n",
    "            #             > 0, 1, 2...\n",
    "\n",
    "            # token_ids : train_dataloader에서 텍스트의 임베딩값을 tensor형식으로 배치사이즈 만큼 가져옴\n",
    "            #            >tensor([[   2, 4893,  517,  ...,    1,    1,    1],\n",
    "            #            >[   2, 1289, 6064,  ...,    1,    1,    1]], dtype=torch.int32) ...\n",
    "            # valid_length : 받아온 token_ids에서 패딩을 제외한 사이즈\n",
    "            #                > tensor([24], dtype=torch.int32)\n",
    "            #                > tensor([81], dtype=torch.int32)\n",
    "            # segment_ids : >tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
    "            #                        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n",
    "            # label : >tensor([[0, 1, 0, 0, 0],\n",
    "            #         >      [0, 0, 1, 0, 0]], dtype=torch.int32)\n",
    "            #         >tensor([[0, 0, 0, 1, 0],\n",
    "            #         >      [0, 0, 1, 0, 0]], dtype=torch.int32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # optimizer.zero_grad() : https://algopoolja.tistory.com/55 참고\n",
    "\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length = valid_length\n",
    "            label = label.long().to(device)\n",
    "            # gpu 연산을 사용할 부분 뒤에는 .to(device) 를 붙여 gpu를 사용\n",
    "\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            # out : token_ids, valid_length, segment_ids를 계산해서 나온 라벨의 softmax 값\n",
    "            # >tensor([[0.2530, 0.1866, 0.1529, 0.2185, 0.1890]], device='cuda:0',\n",
    "            # >grad_fn=<SoftmaxBackward>)\n",
    "            # >tensor([[0.3168, 0.1648, 0.1788, 0.2057, 0.1338]], device='cuda:0',\n",
    "            # >grad_fn=<SoftmaxBackward>)\n",
    "\n",
    "            if num_class == 5:\n",
    "                loss = one_hot_ce_loss(out, label)\n",
    "            elif num_class == 2:\n",
    "                loss = loss_fn(out, label)\n",
    "            elif num_class == 3:\n",
    "                loss = one_hot_ce_loss(out, label)\n",
    "            else:\n",
    "                print('wrong loss function\\n')\n",
    "                sys.exit()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            # loss.item() : 손실값\n",
    "\n",
    "            loss.backward()\n",
    "            # loss.backward() : 오차(error)를 역전파하기 위해 사용\n",
    "            # https://tutorials.pytorch.kr/beginner/blitz/neural_networks_tutorial.html 참고\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            # clip_grad_norm_ : gradient exploding을 방지하여 학습의 안정화를 도모하기 위해 사용하는 방법\n",
    "            # torch.nn.Module.parameters() : 모듈의 파라메터들을 iterator로 반환 --> 무슨 말인지 모르겠음\n",
    "            # https://easy-going-programming.tistory.com/11 참고\n",
    "\n",
    "            optimizer.step()\n",
    "            # optimizer.step() : 매개변수가 갱신\n",
    "\n",
    "            scheduler.step()  \n",
    "            # Update learning rate schedule\n",
    "\n",
    "            # train_acc += calc_accuracy_2(out, label)\n",
    "            trained_value, predicted_value, actual_value = calc_accuracy_3(out, label)\n",
    "            train_predict_list.append(predicted_value)\n",
    "            # predict_value_lsit에 예측한 결과 담기\n",
    "\n",
    "            train_actual_list.append(actual_value)\n",
    "            # actual_value_list에 실제 결과 담기\n",
    "\n",
    "    #         label.tolist()\n",
    "    #         actual_value_list\n",
    "\n",
    "            if num_class == 5:\n",
    "                train_acc += trained_value\n",
    "            elif num_class == 2:\n",
    "                train_acc += trained_value\n",
    "            elif num_class == 3:\n",
    "                train_acc += trained_value\n",
    "\n",
    "            else:\n",
    "                print('wrong calc accuracy\\n')\n",
    "                sys.exit()\n",
    "        \n",
    "        train_f1 = f1_score(train_actual_list, train_predict_list, average='weighted')\n",
    "        train_acc = train_acc / (batch_id + 1)\n",
    "        train_loss = train_loss / (batch_id + 1)\n",
    "        train_acc_list.append(train_acc / (batch_id + 1))\n",
    "        train_loss_list.append(train_loss / (batch_id + 1))\n",
    "        wandb.log({\"train_f1\":train_f1, \"train_acc\":train_acc, \"train_loss\":train_loss})\n",
    "        \n",
    "        print(\"epoch {} train acc {} F1 score {}\".format(e + 1, train_acc / (batch_id + 1), train_f1))\n",
    "\n",
    "        model.eval()\n",
    "        # 모델 평가모드\n",
    "\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length = valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "            if num_class == 5:\n",
    "                loss = one_hot_ce_loss(out, label)\n",
    "            elif num_class == 2:\n",
    "                loss = loss_fn(out, label)\n",
    "            elif num_class == 3:\n",
    "                loss = one_hot_ce_loss(out, label)\n",
    "            else:\n",
    "                print('wrong loss function\\n')\n",
    "                sys.exit()\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            test_value, predicted_value, actual_value = calc_accuracy_3(out, label)\n",
    "            test_predict_list.append(predicted_value)\n",
    "            # predict_value_lsit에 예측한 결과 담기\n",
    "\n",
    "            test_actual_list.append(actual_value)\n",
    "            # actual_value_list에 실제 결과 담기\n",
    "\n",
    "            if num_class == 5:\n",
    "                test_acc += test_value\n",
    "            elif num_class == 2:\n",
    "                test_acc += test_value\n",
    "            elif num_class == 3:\n",
    "                test_acc += test_value\n",
    "            else:\n",
    "                print('wrong calc accuracy\\n')\n",
    "                sys.exit()\n",
    "                \n",
    "        test_f1 = f1_score(test_actual_list, test_predict_list, average='weighted')\n",
    "        test_acc = test_acc / (batch_id + 1)\n",
    "        test_loss = test_loss / (batch_id + 1)\n",
    "        test_acc_list.append(test_acc / (batch_id + 1))\n",
    "        test_loss_list.append(test_loss / (batch_id + 1))\n",
    "        wandb.log({\"test_f1\":test_f1, \"test_acc\":test_acc, \"test_loss\":test_loss})\n",
    "                      \n",
    "        print(\"epoch {} test acc {} F1 score {}\".format(e + 1, test_acc / (batch_id + 1), test_f1))\n",
    "\n",
    "\n",
    "    #     if graph:\n",
    "    #         train_x_values = range(1, len(train_acc_list) + 1)\n",
    "    #         train_y_values = train_acc_list\n",
    "    #         test_x_values = range(1, len(test_acc_list) + 1)\n",
    "    #         test_y_values = test_acc_list\n",
    "    #         train_loss_x_values = range(1, len(train_loss_list) + 1)\n",
    "    #         train_loss_y_values = train_loss_list\n",
    "    #         test_loss_x_values = range(1, len(test_loss_list) + 1)\n",
    "    #         test_loss_y_values = test_loss_list\n",
    "\n",
    "    #         plt.figure(1)\n",
    "    #         plt.plot(train_x_values, train_y_values, label='train accuracy')\n",
    "    #         plt.plot(test_x_values, test_y_values, label='test accuracy')\n",
    "    #         plt.xlabel('epoch', fontproperties=fprop, fontsize=15)\n",
    "    #         plt.ylabel('accuracy', fontproperties=fprop, fontsize=15)\n",
    "    #         plt.ylim(0.0, 1.1)\n",
    "    #         plt.title(graph_title + ' accuracy', fontproperties=fprop, fontsize=20)\n",
    "    #         # plt.title(file, fontsize=20)\n",
    "    #         plt.legend()\n",
    "    #         plt.savefig('./result/' + image_name + '_' + str(batch_size) + '_'+ str(drop_out_rate) + '_' + str(learning_rate) + '_' + str(max_acc) + '_accuracy.png')\n",
    "\n",
    "    #         plt.figure(2)\n",
    "    #         plt.plot(train_loss_x_values, train_loss_y_values, label='train loss')\n",
    "    #         plt.plot(test_loss_x_values, test_loss_y_values, label='test loss')\n",
    "    #         plt.xlabel('epoch', fontproperties=fprop, fontsize=15)\n",
    "    #         plt.ylabel('loss', fontproperties=fprop, fontsize=15)\n",
    "    #         plt.title(graph_title + ' loss', fontproperties=fprop, fontsize=20)\n",
    "    #         # plt.title(file, fontsize=20)\n",
    "    #         plt.legend()\n",
    "    #         plt.savefig('./result/'  + image_name + '_' + str(batch_size) + '_'+ str(drop_out_rate) + '_' + str(learning_rate) + '_' + str(min_loss) + '_loss.png')\n",
    "\n",
    "    if savemode:\n",
    "        torch.save(model.state_dict(), './' + best_model_name)\n",
    "                           \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9438b0cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5 class accuracy\n",
    "def calc_accuracy_1(X, Y):\n",
    "    # e.g. X = tensor([[0.2003, 0.1883, 0.1572, 0.2594, 0.1948]], device='cuda:0',\n",
    "    #                grad_fn=<SoftmaxBackward>)\n",
    "    #      Y = tensor([[0, 0, 1, 0, 0]], device='cuda:0')\n",
    "    \n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    # e.g. max_vals = 0.2594\n",
    "    #      max_indices = 3\n",
    "    \n",
    "    predicted_value = max_indices[0].tolist()\n",
    "    # F1 score 을 위해 예측값 담기\n",
    "    \n",
    "    _, max_Y = torch.max(Y, 1)\n",
    "    actual_value = max_Y[0].tolist()\n",
    "    # F1 score 을 위해 실제값 담기\n",
    "    \n",
    "    encoding = []\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        if int(max_indices[i]) == 0:\n",
    "            encoding.append([1, 0, 0, 0, 0])\n",
    "        elif int(max_indices[i]) == 1:\n",
    "            encoding.append([0, 1, 0, 0, 0])\n",
    "        elif int(max_indices[i]) == 2:\n",
    "            encoding.append([0, 0, 1, 0, 0])\n",
    "        elif int(max_indices[i]) == 3:\n",
    "            encoding.append([0, 0, 0, 1, 0])\n",
    "        elif int(max_indices[i]) == 4:\n",
    "            encoding.append([0, 0, 0, 0, 1])\n",
    "    # tensor에서 가장 높은 값을 인덱스를 원-핫 인코딩 형식으로 인코딩\n",
    "    \n",
    "    encoding = torch.tensor(encoding).to(device)\n",
    "    # e.g. encoding : >tensor([[0, 0, 0, 1, 0]], device='cuda:0')\n",
    "    \n",
    "    cor_matrix = (encoding == Y).tolist()\n",
    "    # Y값 즉, 실제 값과 같으면 리스트로 변환\n",
    "    # e.g. [[True, True, False, False, True]]\n",
    "    \n",
    "    correct = 0.\n",
    "\n",
    "    for j in range(len(Y)):\n",
    "        if all(cor_matrix[j]):\n",
    "            # 함수 all 은 iterable 내의 모든 요소가 참이거나 혹은 iterable 이 비어 있다면 True 를 반환하고, \n",
    "            # 그 외의 경우에는 False 를 반환\n",
    "            # https://codepractice.tistory.com/87 참고\n",
    "            # e.g. cor_matrix[0] : [True, True, False, False, True]\n",
    "            # e.g. all(cor_matrix[0]) : False\n",
    "            \n",
    "            correct += 1.\n",
    "\n",
    "    train_acc = correct / len(Y)\n",
    "    # train_acc : 실제값과 예측값이 같으면 1 아니면 0을 반환\n",
    "\n",
    "    return train_acc, predicted_value, actual_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6133d05d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3 class accuracy\n",
    "def calc_accuracy_3(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    \n",
    "    predicted_value = max_indices[0].tolist()\n",
    "     # F1 score 을 위해 예측값 담기\n",
    "\n",
    "    _, max_Y = torch.max(Y, 1)\n",
    "    actual_value = max_Y[0].tolist()\n",
    "     # F1 score 을 위해 실제값 담기\n",
    "    \n",
    "    encoding = []\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        if int(max_indices[i]) == 0:\n",
    "            encoding.append([1, 0, 0])\n",
    "        elif int(max_indices[i]) == 1:\n",
    "            encoding.append([0, 1, 0])\n",
    "        elif int(max_indices[i]) == 2:\n",
    "            encoding.append([0, 0, 1])\n",
    "    encoding = torch.tensor(encoding).to(device)\n",
    "    # tensor에서 가장 높은 값을 인덱스를 원-핫 인코딩 형식으로 인코딩\n",
    "\n",
    "    cor_matrix = (encoding == Y).tolist()\n",
    "    correct = 0.\n",
    "\n",
    "    for j in range(len(Y)):\n",
    "        if all(cor_matrix[j]):\n",
    "            # 함수 all 은 iterable 내의 모든 요소가 참이거나 혹은 iterable 이 비어 있다면 True 를 반환하고, \n",
    "            # 그 외의 경우에는 False 를 반환\n",
    "            # https://codepractice.tistory.com/87 참고\n",
    "            # e.g. cor_matrix[0] : [True, True, False, False, True]\n",
    "            # e.g. all(cor_matrix[0]) : False\n",
    "            correct += 1.\n",
    "\n",
    "    train_acc = correct / len(Y)\n",
    "    # train_acc : 실제값과 예측값이 같으면 1 아니면 0을 반환\n",
    "    \n",
    "    return train_acc, predicted_value, actual_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8446cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 class accuracy\n",
    "def calc_accuracy(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1e7f268",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">helpful-dream-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/junoe/bax_KoBERT\" target=\"_blank\">https://wandb.ai/junoe/bax_KoBERT</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/junoe/bax_KoBERT/runs/3giraz68\" target=\"_blank\">https://wandb.ai/junoe/bax_KoBERT/runs/3giraz68</a><br/>\n",
       "                Run data is saved locally in <code>/home/khd/BERT_BMC/wandb/run-20210919_025427-3giraz68</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.004807239136582147 F1 score 0.48178183894917187\n",
      "epoch 1 test acc 0.01849112426035503 F1 score 0.39903846153846156\n",
      "epoch 2 train acc 0.006009048920727684 F1 score 0.5787884445651436\n",
      "epoch 2 test acc 0.02551775147928994 F1 score 0.7029171808583572\n",
      "epoch 3 train acc 0.0066217362616646245 F1 score 0.6385220024482959\n",
      "epoch 3 test acc 0.025887573964497042 F1 score 0.7880673248320307\n",
      "epoch 4 train acc 0.007163728909416533 F1 score 0.6592101993996007\n",
      "epoch 4 test acc 0.02403846153846154 F1 score 0.7628402366863904\n",
      "epoch 5 train acc 0.007399377886699972 F1 score 0.7171970696547785\n",
      "epoch 5 test acc 0.026257396449704144 F1 score 0.801724429416737\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    data = read_data(path='./data/')\n",
    "\n",
    "    ## exectue model\n",
    "    model(data=data, savemode=True,  column_name=column_name, labels=labels,  best_model_name=best_model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f13ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
